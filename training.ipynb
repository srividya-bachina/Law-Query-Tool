{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-7OkJ0TqU85",
    "outputId": "d43c059b-09f8-4968-aa71-be0fd0971cc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.0.1.tar.gz (731 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.8/731.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
      "Building wheels for collected packages: pyngrok\n",
      "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyngrok: filename=pyngrok-7.0.1-py3-none-any.whl size=21122 sha256=5593f24e7843b2bb89fe07e068dbdbc36c8b4da7ab6f0b4b666995d26901229f\n",
      "  Stored in directory: /root/.cache/pip/wheels/3b/32/0e/27789b6fde02bf2b320d6f1a0fd9e1354b257c5f75eefc29bc\n",
      "Successfully built pyngrok\n",
      "Installing collected packages: pyngrok\n",
      "Successfully installed pyngrok-7.0.1\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libfontenc1 libxfont2 libxkbfile1 libxtst6 libxxf86dga1 x11-xkb-utils xfonts-base\n",
      "  xfonts-encodings xfonts-utils xserver-common\n",
      "Suggested packages:\n",
      "  mesa-utils\n",
      "The following NEW packages will be installed:\n",
      "  libfontenc1 libxfont2 libxkbfile1 libxtst6 libxxf86dga1 x11-utils x11-xkb-utils xfonts-base\n",
      "  xfonts-encodings xfonts-utils xserver-common xvfb\n",
      "0 upgraded, 12 newly installed, 0 to remove and 15 not upgraded.\n",
      "Need to get 8,045 kB of archives.\n",
      "After this operation, 12.8 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n",
      "Fetched 8,045 kB in 0s (31.1 MB/s)\n",
      "Selecting previously unselected package libfontenc1:amd64.\n",
      "(Reading database ... 120882 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
      "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
      "Selecting previously unselected package libxfont2:amd64.\n",
      "Preparing to unpack .../01-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
      "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
      "Selecting previously unselected package libxkbfile1:amd64.\n",
      "Preparing to unpack .../02-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
      "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
      "Selecting previously unselected package libxtst6:amd64.\n",
      "Preparing to unpack .../03-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
      "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Selecting previously unselected package libxxf86dga1:amd64.\n",
      "Preparing to unpack .../04-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
      "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
      "Selecting previously unselected package x11-utils.\n",
      "Preparing to unpack .../05-x11-utils_7.7+5build2_amd64.deb ...\n",
      "Unpacking x11-utils (7.7+5build2) ...\n",
      "Selecting previously unselected package x11-xkb-utils.\n",
      "Preparing to unpack .../06-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
      "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
      "Selecting previously unselected package xfonts-encodings.\n",
      "Preparing to unpack .../07-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
      "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
      "Selecting previously unselected package xfonts-utils.\n",
      "Preparing to unpack .../08-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
      "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
      "Selecting previously unselected package xfonts-base.\n",
      "Preparing to unpack .../09-xfonts-base_1%3a1.0.5_all.deb ...\n",
      "Unpacking xfonts-base (1:1.0.5) ...\n",
      "Selecting previously unselected package xserver-common.\n",
      "Preparing to unpack .../10-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n",
      "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
      "Selecting previously unselected package xvfb.\n",
      "Preparing to unpack .../11-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n",
      "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
      "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
      "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
      "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
      "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
      "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
      "Setting up x11-xkb-utils (7.7+5build4) ...\n",
      "Setting up xfonts-utils (1:7.7+6build2) ...\n",
      "Setting up xfonts-base (1:1.0.5) ...\n",
      "Setting up x11-utils (7.7+5build2) ...\n",
      "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
      "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install pyngrok\n",
    "!apt-get install -y xvfb x11-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Og7t4URRchk_",
    "outputId": "b75aa282-120c-4b9a-b8c5-f6b0babd169d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9pQI2imm2Zz",
    "outputId": "e8b0c4d8-47ea-4f02-a017-a90da3f629e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzZLUCLUccrs"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Lambda, Embedding, Dense, GlobalAveragePooling1D, Input\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBO7WFbmccry"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load padded_sequences and embedding_matrix from disk\n",
    "padded_X = np.load('/content/drive/MyDrive/padded_X.npy')\n",
    "with open('/content/drive/MyDrive/embedding_matrix.pkl', 'rb') as f:\n",
    "    embedding_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "By81uEER4XOZ",
    "outputId": "3cad8f70-8221-4bb8-be76-9c54289fab9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Case_Type_str\n",
      "0          Administrative Law\n",
      "1          Administrative Law\n",
      "2          Administrative Law\n",
      "3          Administrative Law\n",
      "4          Administrative Law\n",
      "...                       ...\n",
      "39150   Workers' Compensation\n",
      "39151   Workers' Compensation\n",
      "39152   Workers' Compensation\n",
      "39153   Workers' Compensation\n",
      "39154   Workers' Compensation\n",
      "\n",
      "[39155 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"/content/drive/MyDrive/y.csv\"\n",
    "\n",
    "# Load data into a DataFrame\n",
    "y = pd.read_csv(file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MffkNdJeccrz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(padded_X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VB7h_0_rgHPH",
    "outputId": "643bb45c-3f01-4d95-e345-5f0dbd88f870"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_train is your categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.fit_transform(y_val)\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)\n",
    "\n",
    "# Reshape y_train_encoded to a column vector\n",
    "y_train_encoded_reshaped = np.array(y_train_encoded).reshape(-1, 1)\n",
    "y_val_encoded_reshaped = np.array(y_val_encoded).reshape(-1, 1)\n",
    "y_test_encoded_reshaped = np.array(y_test_encoded).reshape(-1, 1)\n",
    "\n",
    "# Use OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_train_onehot = onehot_encoder.fit_transform(y_train_encoded_reshaped)\n",
    "y_val_onehot = onehot_encoder.fit_transform(y_val_encoded_reshaped)\n",
    "y_test_onehot = onehot_encoder.fit_transform(y_test_encoded_reshaped)\n",
    "y_train_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXOfgUyHccr0",
    "outputId": "40192083-4e33-48d7-be9e-e9cfd35dddfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27408, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zPKis2Gccr1",
    "outputId": "af98b5fa-a125-49da-e9d6-a83e3bf7c9b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27408, 76)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBzsaz8Fccr2"
   },
   "outputs": [],
   "source": [
    "vocabulary_size, embedding_dimension = embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UhNzIxXJx0Sj",
    "outputId": "592c7bb7-e3b2-4a9a-8895-5f2a7cf34a77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 268458\n",
      "Embedding dimension: 200\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocabulary size: {vocabulary_size}')\n",
    "print(f'Embedding dimension: {embedding_dimension}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfNoTkkXZy2D"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSuQbW90fObd",
    "outputId": "98964612-107a-4214-d056-07707334180f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "857/857 [==============================] - 58s 59ms/step - loss: 3.8860 - accuracy: 0.0903 - val_loss: 3.5531 - val_accuracy: 0.1514\n",
      "Epoch 2/20\n",
      "857/857 [==============================] - 20s 24ms/step - loss: 3.3988 - accuracy: 0.1927 - val_loss: 3.2544 - val_accuracy: 0.2231\n",
      "Epoch 3/20\n",
      "857/857 [==============================] - 15s 17ms/step - loss: 2.8553 - accuracy: 0.3024 - val_loss: 3.4070 - val_accuracy: 0.2079\n",
      "Epoch 4/20\n",
      "857/857 [==============================] - 14s 16ms/step - loss: 2.4449 - accuracy: 0.3912 - val_loss: 2.8712 - val_accuracy: 0.3181\n",
      "Epoch 5/20\n",
      "857/857 [==============================] - 14s 17ms/step - loss: 2.0210 - accuracy: 0.4879 - val_loss: 2.9059 - val_accuracy: 0.3233\n",
      "Epoch 6/20\n",
      "857/857 [==============================] - 14s 16ms/step - loss: 1.6004 - accuracy: 0.5870 - val_loss: 2.9997 - val_accuracy: 0.3395\n",
      "Epoch 7/20\n",
      "857/857 [==============================] - 13s 15ms/step - loss: 1.1923 - accuracy: 0.6926 - val_loss: 3.1705 - val_accuracy: 0.3438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7dec5c605ed0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "num_classes = 76\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=embedding_dimension, input_length=200))\n",
    "model.add(LSTM(150))  # Adjust the number of LSTM units as needed\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train_onehot, epochs=20, batch_size=32, validation_data=(X_val, y_val_onehot), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Rb4g2GEVZr3",
    "outputId": "07ba45d2-390c-49d9-b38a-ee291b25b364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 1s 4ms/step\n",
      "Test Accuracy: 0.3459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_test_decoded = np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "# Get model predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_decoded = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test_decoded, y_pred_decoded)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMvL-hpwqwNM"
   },
   "outputs": [],
   "source": [
    "model.save_weights('/content/drive/MyDrive/model1_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "VwS6Vb-3EBhZ",
    "outputId": "b5ce0767-1aab-4915-e578-401650a20df1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39155 entries, 0 to 39154\n",
      "Data columns (total 2 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Case_Type              39155 non-null  int64 \n",
      " 1   processed_description  39154 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 611.9+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = \"/content/drive/MyDrive/data.csv\"\n",
    "\n",
    "# Load data into a DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "data = data.drop(columns=['Unnamed: 0', 'Case_description'], axis=1)\n",
    "data['Case_Type'] = data['Case_Type'].astype(int)\n",
    "\n",
    "# Display the DataFrame\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "6uiyZTUGIVeX"
   },
   "outputs": [],
   "source": [
    "# Create an embedding layer\n",
    "embedding_layer = Embedding(\n",
    "    input_dim = vocabulary_size,\n",
    "    output_dim = embedding_dimension,\n",
    "    embeddings_initializer=Constant(embedding_matrix),\n",
    "    input_length= embedding_dimension,\n",
    "    trainable=True  # set to True if you want to fine-tune embeddings during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "MHd8Bl3BAtWQ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import LayerNormalization, MultiHeadAttention, Dense, LSTM, Bidirectional\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rnn_units, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        # RNN as Encoder\n",
    "        self.rnn_encoder = Bidirectional(LSTM(units=rnn_units, return_sequences=True))\n",
    "\n",
    "        # LSTM as Decoder\n",
    "        self.lstm_decoder = LSTM(units=rnn_units, return_sequences=True)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "iPcrP96vH-FZ"
   },
   "outputs": [],
   "source": [
    "# Create the model using TransformerBlock\n",
    "num_heads = 2\n",
    "ff_dim = 32\n",
    "rnn_units = 64\n",
    "\n",
    "transformer_block = TransformerBlock(embedding_dimension, num_heads, ff_dim, rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Cy9bd7NjIDtx"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "inputs = Input(shape=(200,))\n",
    "embedded_inputs = Embedding(\n",
    "    input_dim=vocabulary_size,\n",
    "    output_dim=embedding_matrix.shape[1],\n",
    "    embeddings_initializer=Constant(embedding_matrix),\n",
    "    input_length=200,\n",
    "    trainable=True\n",
    ")(inputs)\n",
    "\n",
    "#inputs = Input(shape=vocabulary_size)\n",
    "embedding_output = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embedding_matrix.shape[1], num_heads=2, ff_dim=32, rnn_units=64)\n",
    "encoder_outputs = transformer_block(embedding_output)\n",
    "outputs = Dense(200, activation='softmax')(encoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "19L8hv2SIG7w",
    "outputId": "50aad935-fa0a-457e-8809-efb94f73a35c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 200, 200)          53691600  \n",
      "                                                                 \n",
      " transformer_block_3 (Trans  (None, 200, 200)          335232    \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 200, 200)          40200     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54067032 (206.25 MB)\n",
      "Trainable params: 54067032 (206.25 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "sentence_model = Model(inputs=inputs, outputs=outputs)\n",
    "sentence_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "eHxuT1h2IKyo",
    "outputId": "2fcd0764-a236-4dc6-9dad-4a40d2b733f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2284/2284 [==============================] - 46s 19ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "2284/2284 [==============================] - 22s 10ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "2284/2284 [==============================] - 22s 10ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "2284/2284 [==============================] - 21s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "2284/2284 [==============================] - 21s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "2284/2284 [==============================] - 21s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "2284/2284 [==============================] - 21s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "2284/2284 [==============================] - 21s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "2284/2284 [==============================] - 21s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "2284/2284 [==============================] - 21s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "2284/2284 [==============================] - 20s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "2284/2284 [==============================] - 21s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "2284/2284 [==============================] - 20s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "2284/2284 [==============================] - 21s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "2284/2284 [==============================] - 20s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "2284/2284 [==============================] - 21s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "2284/2284 [==============================] - 21s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "2284/2284 [==============================] - 20s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "2284/2284 [==============================] - 20s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "2284/2284 [==============================] - 20s 9ms/step - loss: nan - accuracy: 3.6486e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7deb7c4f1480>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "sentence_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "sentence_model.fit(X_train, X_train, epochs=20, batch_size=12, validation_data=(X_val, X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "q5CyP3foBfBF"
   },
   "outputs": [],
   "source": [
    "sentence_model.save_weights('/content/drive/MyDrive/model2_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "agr6JXJ3YND2",
    "outputId": "7a0341c3-0f72-4e6a-a86d-a15f96facc47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "sentence_model.save(\"/content/drive/MyDrive/model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYF3UiAeYlFD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
